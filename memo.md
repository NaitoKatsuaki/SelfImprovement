# パイプライン
各処理をマルチスレッド・マルチコアで動かし、スループット(全体の処理速度)を向上させる目的。
各処理ごとにバッファ(キュー構造)を持たせておいて、データを逐次的に受け渡す。
各処理のことを、この文脈では"ステージ"という。
また、まさにいくつかパイプがあって、水が途中で別のパイプに流れていく様子と、処理フローを可視化した図が似ているので、パイプライン処理というのだろう。
![alt text](img\pipeLine.png)

# SIMD
Single Instruction Multiple Dataの略
1つの命令で独立な複数個のデータを一気に処理するためのCPUハードウェアの機能(アーキテクチャレベルの命令セット)
これを利用可能にするのが、コンパイラであり、コンパイラに検知させるためのコードを書く必要がある。
最近のコンパイラなら自動検知してある程度最適なSIMD命令でやるようにしてくれる。
例えば可読性向上のためベクトルの和ではなく、forループで処理している部分も自動ベクトル化してくれるらしい。

# メモリの帯域幅とキャッシュ
CPU(やGPU)がメインメモリ(DRAM)とデータをやり取りする際の通り道の広さと速さを表す指標。
**1秒間にメモリから転送できるデータ量のこと**  
メモリ帯域幅[byte/sec] = バス幅 × メモリクロック周波数[処理/sec] × 転送回数 / 8  
たとえばDDR4メモリの場合：  
メモリクロック：2400 MHz  
転送方式：DDR（Double Data Rate）＝1クロックで2回転送  
バス幅：64 bit（1チャンネル）  
とすると、  
帯域幅 = 64bit × 2400MHz × 2 / 8 = 38,400 MB/s ≒ 38.4 GB/s  
これが重要になる理由：  
CPUが高性能で高速に演算できたとしても、メモリからデータをとってくる速度が追い付かないと、待ち時間(メモリボトルネック)が発生するため。⇒メモリウォール問題という。

とても良いところに目をつけています。
キャッシュ階層構造とメモリ帯域幅の関係を理解すると、
なぜCPUが「理論性能どおりに動かないか」や「SIMDが飽和する理由」が一気に見えてきます。

🧩 1. CPUメモリ階層の全体像

CPUのデータアクセスは、実際には次のような階層構造を通ります：
```plantuml
レジスタ  →  L1キャッシュ  →  L2キャッシュ  →  L3キャッシュ  →  メインメモリ（DRAM）
   ↑                                                          ↓
  (数ナノ秒)                                           (数百ナノ秒)
  ```

| 階層               | 容量        | レイテンシ（目安） | 帯域幅（目安）      |
| ---------------- | --------- | --------- | ------------ |
| **レジスタ**         | 数百B       | 1サイクル未満   | 数TB/s級       |
| **L1キャッシュ**      | 32KB前後    | 約1ns      | 数百GB/s〜1TB/s |
| **L2キャッシュ**      | 256〜512KB | 約3〜4ns    | 数百GB/s       |
| **L3キャッシュ**      | 数MB〜数十MB  | 約10ns     | 数十〜百GB/s     |
| **メインメモリ（DRAM）** | GB〜数百GB   | 約100ns    | 数十GB/s       |

※レイテンシ：データを取りに行って戻ってくるまでの時間
※帯域幅：1秒間にどれだけのデータをやり取りできるか

⚙️ 2. キャッシュの役割：帯域の“差”を埋める

CPUとメモリの速度差は100倍以上あります。
そのため、直接メモリにアクセスしていたらCPUはほとんど待機状態。

👉 これを防ぐために、

最近アクセスしたデータを**キャッシュ（L1〜L3）**に保持し、

キャッシュヒットすれば高速に再利用、

キャッシュミスしたら下位層（L2/L3/DRAM）から読み込み、
という仕組みで帯域幅のボトルネックを段階的に緩和しています。

## キャッシュヒット率を上げるために
| 最適化方向                | 意図                         | 具体策                                       |
| -------------------- | -------------------------- | ----------------------------------------- |
| **空間的局所性**           | 連続データを扱うことでキャッシュラインを無駄にしない | 配列アクセス順を線形にする                             |
| **時間的局所性**           | 同じデータを繰り返し使う               | ループ内で使い回す／ブロッキング                          |
| **キャッシュブロッキング**      | 一度に処理するデータをキャッシュに収まるサイズに制限 | 画像処理のタイル化など                               |
| **プレフェッチ**           | 先読みでキャッシュミスを減らす            | `_mm_prefetch()` / `__builtin_prefetch()` |
| **アラインメント最適化**       | キャッシュライン境界を跨がない            | 32/64byte境界にメモリ確保                         |
| **NUMA意識（マルチCPU環境）** | メモリの物理距離による帯域差を回避          | “ローカルメモリ”にデータを置く                          |

## まとめ
| 観点           | 内容                         |
| ------------ | -------------------------- |
| **キャッシュ階層**  | L1〜L3とDRAMで速度差100倍以上       |
| **帯域幅の意味**   | 「各層が1秒間に処理できるデータ量」         |
| **性能ボトルネック** | 計算性能よりデータ転送速度が支配的になる       |
| **最適化の方向性**  | データ局所性・キャッシュヒット率の最大化       |
| **SIMDとの関係** | キャッシュに乗ってこそSIMDが最大性能を発揮できる |
